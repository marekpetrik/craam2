# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' Computes the maximum distribution subject to L1 constraints
#'
#' @param value Random variable (objective)
#' @param reference_dst Reference distribution of the same size as value
#' @param budget Maximum L1 distance from the reference dst
#'
#' @returns A list with dst as the worstcase distribution,
NULL

#' Computes average value at risk
#'
#' @param value Random variable (as a vector over realizations)
#' @param reference_dst Reference distribution of the same size as value
#' @param alpha Confidence value. 0 is worst case, 1 is average
#'
#' @returns A list with dst as the distorted distribution,
NULL

worstcase_l1 <- function(value, reference_dst, budget) {
    .Call(`_rcraam_worstcase_l1`, value, reference_dst, budget)
}

avar <- function(value, reference_dst, alpha) {
    .Call(`_rcraam_avar`, value, reference_dst, alpha)
}

pack_actions <- function(mdp) {
    .Call(`_rcraam_pack_actions`, mdp)
}

mdp_clean <- function(mdp) {
    .Call(`_rcraam_mdp_clean`, mdp)
}

#' Solves a plain Markov decision process.
#'
#' This method supports only deterministic policies. See solve_mdp_rand for a
#' method that supports randomized policies.
#'
#' @param mdp A dataframe representation of the MDP. Each row
#'            represents a single transition from one state to another
#'            after taking an action a. The columns are:
#'            idstatefrom, idaction, idstateto, probability, reward
#' @param discount Discount factor in [0,1]
#' @param algorithm One of "mpi", "vi", "vi_j", "vi_g", "pi". Also supports "lp"
#'           when Gurobi is properly installed
#' @param policy_fixed States for which the  policy should be fixed. This
#'          should be a dataframe with columns idstate and idaction. The policy
#'          is optimized only for states that are missing, and the fixed policy
#'          is used otherwise. Both indices are 0-based.
#' @param maxresidual Residual at which to terminate
#' @param iterations Maximum number of iterations
#' @param timeout Maximum number of secods for which to run the computation
#' @param value_init A  dataframe that contains the initial value function used
#'          to initialize the method. The columns should be idstate and value.
#'          Any states that are not provided are initialized to 0.
#' @param pack_actions Whether to remove actions with no transition probabilities,
#'          and rename others for the same state to prevent gaps. The policy
#'          for the original actions can be recovered using ``action_map'' frame
#'          in the result
#' @param output_tran Whether to construct and return a matrix of transition
#'          probabilites and a vector of rewards
#' @param show_progress Whether to show a progress bar during the computation.
#'         0 means no progress, 1 is progress bar, and 2 is a detailed report
#' @return A list with value function policy and other values
solve_mdp <- function(mdp, discount, algorithm = "mpi", policy_fixed = NULL, maxresidual = 10e-4, iterations = 10000L, timeout = 300, value_init = NULL, pack_actions = FALSE, output_tran = FALSE, show_progress = 1L) {
    .Call(`_rcraam_solve_mdp`, mdp, discount, algorithm, policy_fixed, maxresidual, iterations, timeout, value_init, pack_actions, output_tran, show_progress)
}

#' Solves a plain Markov decision process with randomized policies.
#'
#' The method can be provided with a randomized policy for some states
#' and the output policy is randomized.
#'
#' @param mdp A dataframe representation of the MDP. Each row
#'            represents a single transition from one state to another
#'            after taking an action a. The columns are:
#'            idstatefrom, idaction, idstateto, probability, reward
#' @param discount Discount factor in [0,1]
#' @param algorithm One of "mpi", "vi", "vi_j", "vi_g", "pi"
#' @param policy_fixed States for which the  policy should be fixed. This
#'         should be a dataframe with columns idstate, idaction, probability.
#'          The policy is optimized only for states that are missing, and the
#'          fixed policy is used otherwise
#' @param maxresidual Residual at which to terminate
#' @param iterations Maximum number of iterations
#' @param timeout Maximum number of secods for which to run the computation
#' @param value_init A  dataframe that contains the initial value function used
#'          to initialize the method. The columns should be idstate and value.
#'          Any states that are not provided are initialized to 0.
#' @param output_tran Whether to construct and return a matrix of transition
#'          probabilites and a vector of rewards
#' @param show_progress Whether to show a progress bar during the computation
#'         0 means no progress, 1 is progress bar, and 2 is a detailed report
#'
#' @return A list with value function policy and other values
solve_mdp_rand <- function(mdp, discount, algorithm = "mpi", policy_fixed = NULL, maxresidual = 10e-4, iterations = 10000L, timeout = 300, value_init = NULL, output_tran = FALSE, show_progress = 1L) {
    .Call(`_rcraam_solve_mdp_rand`, mdp, discount, algorithm, policy_fixed, maxresidual, iterations, timeout, value_init, output_tran, show_progress)
}

#' Computes the function for the MDP for the given value function and discount factor
#'
#' @param mdp A dataframe representation of the MDP. Each row
#'            represents a single transition from one state to another
#'            after taking an action a. The columns are:
#'            idstatefrom, idaction, idstateto, probability, reward
#' @param discount Discount factor in [0,1]
#' @param valuefunction A dataframe representation of the value function. Each row
#'             represents a state. The columns must be idstate, value
#'
#' @return Dataframe with idstate, idaction, qvalue columns
compute_qvalues <- function(mdp, discount, valuefunction) {
    .Call(`_rcraam_compute_qvalues`, mdp, discount, valuefunction)
}

#' Solves a robust Markov decision process with state-action rectangular
#' ambiguity sets. The worst-case is computed with the MDP transition
#' probabilities treated as nominal values.
#'
#' NOTE: The algorithms: pi, mpi may cycle infinitely without converging to a solution,
#' when solving a robust MDP.
#' The algorithms ppi and mppi are guaranteed to converge to an optimal solution.
#'
#' Important: Worst-case transitions are allowed only to idstateto states that
#' are provided in the mdp dataframe, even when the transition
#' probability to those states is 0.
#'
#' @param mdp A dataframe representation of the MDP. Each row
#'            represents a single transition from one state to another
#'            after taking an action a. The columns are:
#'            idstatefrom, idaction, idstateto, probability, reward
#' @param discount Discount factor in [0,1]
#' @param nature Algorithm used to select the robust outcome. See details for options.
#' @param nature_par Parameters for the nature. Varies depending on the nature.
#'                   See details for options.
#' @param algorithm One of "ppi", "mppi", "vppi", "mpi", "vi", "vi_j", "vi_g", "pi". MPI and PI may
#'           may not converge
#' @param policy_fixed States for which the  policy should be fixed. This
#'          should be a dataframe with columns idstate and idaction. The policy
#'          is optimized only for states that are missing, and the fixed policy
#'          is used otherwise
#' @param maxresidual Residual at which to terminate
#' @param iterations Maximum number of iterations
#' @param timeout Maximum number of secods for which to run the computation
#' @param value_init A  dataframe that contains the initial value function used
#'          to initialize the method. The columns should be idstate and value.
#'          Any states that are not provided are initialized to 0.
#' @param pack_actions Whether to remove actions with no transition probabilities,
#'          and rename others for the same state to prevent gaps. The policy
#'          for the original actions can be recovered using ``action_map'' frame
#'          in the result
#' @param output_tran Whether to construct and return a matrix of transition
#'          probabilites and a vector of rewards
#' @param show_progress Whether to show a progress bar during the computation.
#'         0 means no progress, 1 is progress bar, and 2 is a detailed report
#'
#' @return A list with value function policy and other values
#'
#' @details
#'
#' The options for nature and the corresponding nature_par are:
#'    \itemize{
#'         \item "l1u" an l1 ambiguity set with the same budget for all s,a.
#'                nature_par is a float number representing the budget
#'         \item "l1" an ambiguity set with different budgets for each s,a.
#'                nature_par is dataframe with idstate, idaction, budget
#'         \item "l1w" an l1-weighted ambiguity set with different weights
#'                      and budgets for each state and action
#'                 nature_par is a list with two elements: budgets, weights.
#'                 budgets must be a dataframe with columns idstate, idaction, budget
#'                 and weights must be a dataframe with columns:
#'                 idstatefrom, idaction, idstateto, weight (for the l1 weighted norms)
#'         \item "evaru" a convex combination of expectation and V@R over
#'                 transition probabilites. Uniform over all states and actions
#'                 nature_par is a list with parameters (alpha, beta). The worst-case
#'                 response is computed as:
#'                 beta * var [z] + (1-beta) * E[z], where
#'                 var is inf{x \in R : P[X <= x] >= alpha}, with alpha = 0 being the
#'                 worst-case.
#'         \item "evaru" a convex combination of expectation and AV@R over
#'                 transition probabilites. Uniform over states
#'                 nature_par is a list with parameters (alpha, beta). The worst-case
#'                 response is computed as:
#'                 beta * var [z] + (1-beta) * E[z], where
#'                 var is AVaR(z,alpha) = 1/alpha * ( E[X I{X <= x_a} ] + x_a (alpha - P[X <= x_a])
#'                 where I is the indicator function and
#'                 x_a = inf{x \in R : P[X <= x] >= alpha} being the
#'                 worst-case.
#'    }
rsolve_mdp_sa <- function(mdp, discount, nature, nature_par, algorithm = "mppi", policy_fixed = NULL, maxresidual = 10e-4, iterations = 10000L, timeout = 300, value_init = NULL, pack_actions = FALSE, output_tran = FALSE, show_progress = 1L) {
    .Call(`_rcraam_rsolve_mdp_sa`, mdp, discount, nature, nature_par, algorithm, policy_fixed, maxresidual, iterations, timeout, value_init, pack_actions, output_tran, show_progress)
}

#' Solves a robust Markov decision process with state-action rectangular
#' ambiguity sets.
#'
#' The worst-case is computed across the outcomes and not
#' the actual transition probabilities.
#'
#' NOTE: The algorithms  mpi and pi may cycle infinitely without converging to a solution,
#' when solving a robust MDP.
#' The algorithms ppi and mppi are guaranteed to converge to an optimal solution.
#'
#'
#' @param algorithm One of "ppi", "mppi", "mpi", "vi", "vi_j", "vi_g", "pi". MPI may
#'           may not converge
#' @param policy_fixed States for which the  policy should be fixed. This
#'          should be a dataframe with columns idstate and idaction. The policy
#'          is optimized only for states that are missing, and the fixed policy
#'          is used otherwise
#' @param maxresidual Residual at which to terminate
#' @param iterations Maximum number of iterations
#' @param timeout Maximum number of secods for which to run the computation
#' @param value_init A  dataframe that contains the initial value function used
#'          to initialize the method. The columns should be idstate and value.
#'          Any states that are not provided are initialized to 0.
#' @param pack_actions Whether to remove actions with no transition probabilities,
#'          and rename others for the same state to prevent gaps. The policy
#'          for the original actions can be recovered using ``action_map'' frame
#'          in the result
#' @param output_tran Whether to construct and return a matrix of transition
#'          probabilites and a vector of rewards
#' @param show_progress Whether to show a progress bar during the computation.
#'         0 means no progress, 1 is progress bar, and 2 is a detailed report
#'
#' @return A list with value function policy and other values
#'
#' @details
#'
#' The options for nature and the corresponding nature_par are:
#'    \itemize{
#'         \item "exp" plain expectation over the outcomes
#'         \item "evaru" a convex combination of expectation and V@R over
#'                 transition probabilites. Uniform over all states and actions
#'                 nature_par is a list with parameters (alpha, beta). The worst-case
#'                 response is computed as:
#'                 beta * var [z] + (1-beta) * E[z], where
#'                 var is \eqn{VaR(z,\alpha) = \inf{x \in R : P[X <= x] >= \alpha}}, with \eqn{\alpha = 0} being the
#'                 worst-case.
#'         \item "eavaru" a convex combination of expectation and AV@R over
#'                 transition probabilites. Uniform over states
#'                 nature_par is a list with parameters (alpha, beta). The worst-case
#'                 response is computed as:
#'                 beta * var [z] + (1-beta) * E[z], where
#'                 var is \eqn{AVaR(z,alpha) =  1/alpha * ( E[X I{X <= x_a} ] + x_a (alpha - P[X <= x_a] )}
#'                 where I is the indicator function and
#'                 \eqn{x_a = \inf{x \in R : P[X <= x] >= \alpha}} being the
#'                 worst-case.
#'    }
rsolve_mdpo_sa <- function(mdpo, discount, nature, nature_par, algorithm = "mppi", policy_fixed = NULL, maxresidual = 10e-4, iterations = 10000L, timeout = 300, value_init = NULL, pack_actions = FALSE, output_tran = FALSE, show_progress = 1L) {
    .Call(`_rcraam_rsolve_mdpo_sa`, mdpo, discount, nature, nature_par, algorithm, policy_fixed, maxresidual, iterations, timeout, value_init, pack_actions, output_tran, show_progress)
}

#' Solves a robust Markov decision process with state rectangular
#' ambiguity sets. The worst-case is computed with the MDP transition
#' probabilities treated as nominal values.
#'
#' NOTE: The algorithms: pi, mpi may cycle infinitely without converging to a solution,
#' when solving a robust MDP.
#' The algorithms ppi and mppi are guaranteed to converge to an optimal solution.
#'
#' Important: Worst-case transitions are allowed only to idstateto states that
#' are provided in the mdp dataframe, even when the transition
#' probability to those states is 0.
#'
#' @param algorithm One of "ppi", "mppi", "mpi", "vi", "vi_j", "v_g", "pi". MPI may
#'           may not converge
#' @param policy_fixed States for which the  policy should be fixed. This
#'          should be a dataframe with columns idstate and idaction. The policy
#'          is optimized only for states that are missing, and the fixed policy
#'          is used otherwise
#' @param maxresidual Residual at which to terminate
#' @param iterations Maximum number of iterations
#' @param timeout Maximum number of secods for which to run the computation
#' @param value_init A  dataframe that contains the initial value function used
#'          to initialize the method. The columns should be idstate and value.
#'          Any states that are not provided are initialized to 0.
#' @param pack_actions Whether to remove actions with no transition probabilities,
#'          and rename others for the same state to prevent gaps. The policy
#'          for the original actions can be recovered using ``action_map'' frame
#'          in the result
#' @param output_tran Whether to construct and return a matrix of transition
#'          probabilites and a vector of rewards
#' @param show_progress Whether to show a progress bar during the computation.
#'         0 means no progress, 1 is progress bar, and 2 is a detailed report
#'
#' @return A list with value function policy and other values
#' @details
#'
#' The options for nature and the corresponding nature_par are:
#'    \itemize{
#'         \item "l1u" an l1 ambiguity set with the same budget for all s.
#'                nature_par is a float number representing the budget
#'         \item "l1" an ambiguity set with different budgets for each s.
#'                nature_par is dataframe with idstate, budget
#'         \item "l1w" an l1-weighted ambiguity set with different weights
#'                      and budgets for each state and action
#'                 nature_par is a list with two elements: budgets, weights.
#'                 budgets must be a dataframe with columns idstate, budget
#'                 and weights must be a dataframe with columns:
#'                 idstatefrom, idaction, idstateto, weight (for the l1 weighted norms)
#'    }
rsolve_mdp_s <- function(mdp, discount, nature, nature_par, algorithm = "mppi", policy_fixed = NULL, maxresidual = 10e-4, iterations = 10000L, timeout = 300, value_init = NULL, pack_actions = FALSE, output_tran = FALSE, show_progress = 1L) {
    .Call(`_rcraam_rsolve_mdp_s`, mdp, discount, nature, nature_par, algorithm, policy_fixed, maxresidual, iterations, timeout, value_init, pack_actions, output_tran, show_progress)
}

#' Solves a robust Markov decision process with state-action rectangular
#' ambiguity sets.
#'
#' The worst-case is computed across the outcomes and not
#' the actual transition probabilities.
#'
#' NOTE: The algorithms  mpi and pi may cycle infinitely without converging to a solution,
#' when solving a robust MDP.
#' The algorithms ppi and mppi are guaranteed to converge to an optimal solution.
#'
#'
#' @param algorithm One of "ppi", "mppi", "mpi", "vi", "vi_j", "vi_g", "pi". MPI may
#'           may not converge
#' @param policy_fixed States for which the  policy should be fixed. This
#'          should be a dataframe with columns idstate and idaction. The policy
#'          is optimized only for states that are missing, and the fixed policy
#'          is used otherwise
#' @param maxresidual Residual at which to terminate
#' @param iterations Maximum number of iterations
#' @param timeout Maximum number of secods for which to run the computation
#' @param value_init A  dataframe that contains the initial value function used
#'          to initialize the method. The columns should be idstate and value.
#'          Any states that are not provided are initialized to 0.
#' @param pack_actions Whether to remove actions with no transition probabilities,
#'          and rename others for the same state to prevent gaps. The policy
#'          for the original actions can be recovered using ``action_map'' frame
#'          in the result
#' @param output_tran Whether to construct and return a matrix of transition
#'          probabilites and a vector of rewards
#' @param show_progress Whether to show a progress bar during the computation.
#'         0 means no progress, 1 is progress bar, and 2 is a detailed report
#'
#' @return A list with value function policy and other values
#'
#' @details
#'
#' The options for nature and the corresponding nature_par are:
#'    \itemize{
#'         \item "exp" plain expectation over the outcomes
#'         \item "eavaru" a convex combination of expectation and AV@R over
#'                 transition probabilites. Uniform over states
#'                 nature_par is a list with parameters (alpha, beta). The worst-case
#'                 response is computed as:
#'                 beta * var [z] + (1-beta) * E[z], where
#'                 var is \eqn{AVaR(z,alpha) =  1/alpha * ( E[X I{X <= x_a} ] + x_a (alpha - P[X <= x_a] )}
#'                 where I is the indicator function and
#'                 \eqn{x_a = \inf{x \in R : P[X <= x] >= \alpha}} being the
#'                 worst-case.
#'    }
rsolve_mdpo_s <- function(mdpo, discount, nature, nature_par, algorithm = "mppi", policy_fixed = NULL, maxresidual = 10e-4, iterations = 10000L, timeout = 300, value_init = NULL, pack_actions = FALSE, output_tran = FALSE, show_progress = 1L) {
    .Call(`_rcraam_rsolve_mdpo_s`, mdpo, discount, nature, nature_par, algorithm, policy_fixed, maxresidual, iterations, timeout, value_init, pack_actions, output_tran, show_progress)
}

set_rcraam_threads <- function(n) {
    invisible(.Call(`_rcraam_set_rcraam_threads`, n))
}

#'  Builds an MDP from samples
#'
mdp_from_samples <- function(samples_frame) {
    .Call(`_rcraam_mdp_from_samples`, samples_frame)
}

mdp_example <- function(name) {
    .Call(`_rcraam_mdp_example`, name)
}

mdp_inventory <- function(params) {
    .Call(`_rcraam_mdp_inventory`, params)
}

mdp_population <- function(capacity, initial, growth_rates_exp, growth_rates_std, rewards, external_mean, external_std, s_growth_model) {
    .Call(`_rcraam_mdp_population`, capacity, initial, growth_rates_exp, growth_rates_std, rewards, external_mean, external_std, s_growth_model)
}

#'
#' Simulates an MDP
#'
#' @param mdp Definition of the MDP
#' @param initial_state The index of the initial state
#' @param policy Assumes a randomized policy as the input
#' @param horizon How many steps to execute for each episode
#' @param episodes Number of episodes to run
#' @param seed Random number generator seed (a number)
#'
#' @return A dataframe with the states, actions, and rewards received
#'
simulate_mdp <- function(mdp, initial_state, policy, horizon, episodes, seed = NULL) {
    .Call(`_rcraam_simulate_mdp`, mdp, initial_state, policy, horizon, episodes, seed)
}

