% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{rsolve_mdpo_sa}
\alias{rsolve_mdpo_sa}
\title{Solves a robust Markov decision process with state-action rectangular
ambiguity sets. The worst-case is computed across the outcomes and not
the actual transition probabilities.}
\usage{
rsolve_mdpo_sa(mdpo, discount, nature, nature_par, algorithm = "mppi",
  policy_fixed = NULL, maxresidual = 0.001, iterations = 10000L,
  timeout = 300, pack_actions = FALSE, output_tran = FALSE,
  show_progress = TRUE)
}
\arguments{
\item{algorithm}{One of "ppi", "mppi", "mpi", "vi", "vi_j", "pi". MPI may
may not converge}

\item{policy_fixed}{States for which the  policy should be fixed. This
should be a dataframe with columns idstate and idaction. The policy
is optimized only for states that are missing, and the fixed policy
is used otherwise}

\item{maxresidual}{Residual at which to terminate}

\item{iterations}{Maximum number of iterations}

\item{timeout}{Maximum number of secods for which to run the computation}

\item{pack_actions}{Whether to remove actions with no transition probabilities,
and rename others for the same state to prevent gaps. The policy
for the original actions can be recovered using ``action_map'' frame
in the result}

\item{output_tran}{Whether to construct and return a matrix of transition
probabilites and a vector of rewards}

\item{show_progress}{Whether to show a progress bar during the computation}
}
\value{
A list with value function policy and other values
}
\description{
NOTE: The algorithms  mpi and pi may cycle infinitely without converging to a solution,
when solving a robust MDP.
The algorithms ppi and mppi are guaranteed to converge to an optimal solution.
}
