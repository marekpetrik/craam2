library(gurobi)
library(Matrix)

confidences <- c(0.5,0.95)#c(0.1,0.3,0.5,0.7,0.9,0.95,0.99)
num_confs = length(confidences)

# normalize the vector to have unit some.
normalize <- function(m){
  m/sum(m)
}

# Compute the average of the values in the vector
average <- function(m){
  sum(m)/length(m)
}

# Plots the dataframe and saves the result as a pdf in figs directory
plot.res <- function(df, filter.methods, file.name){
  # Base Plot
  gg <- ggplot(df, aes(x=confidence, y=return, group=method)) +
    geom_line(aes(linetype=method)) + geom_point(aes(shape=method)) +
    theme_light() +
    #scale_y_continuous(trans='log2') +
    theme(legend.position="bottom", legend.direction = "horizontal", legend.box="horizontal", legend.background
          = element_rect(fill="gray95", size=0.2, linetype="solid", color ="black")) +
    scale_shape_manual(values=seq(0,length(filter.methods))) +
    labs(title="", subtitle = "", y="Guaranteed Return", x=TeX("Confidence Level: $1-\\delta$")) +
    labs(shape="Method", linetype="Method")

  ggsave(paste("figs/",file.name,".pdf",sep=""), plot=gg, width = 7, height = 5,  device='pdf')
  plot(gg)
}

# Plots the dataframe and saves the result as a pdf in figs directory
plot.res.manual <- function(df, filter.methods, file.name, shapes, linetypes){
  cat("plot manual invoked")
  # Base Plot
  gg <- ggplot(df, aes(x=confidence, y=return, group=method)) +
    geom_line(aes(linetype=method)) + geom_point(aes(shape=method)) +
    scale_shape_manual(values=shapes)+
    #scale_linetype_manual(values=linetypes) +
    theme_light() +
    theme(legend.position="bottom", legend.direction = "horizontal", legend.box="horizontal", legend.background
          = element_rect(fill="gray95", size=0.2, linetype="solid", color ="black")) +
    #scale_shape_manual(values=seq(0,length(filter.methods))) +
    labs(title="", subtitle = "", y="Guaranteed Return", x=TeX("Confidence Level: $1-\\delta$")) +
    labs(shape="Method", linetype="Method")

  ggsave(paste("figs/",file.name,".pdf",sep=""), plot=gg, width = 7, height = 5,  device='pdf')
  plot(gg)
}

# For each method & required confidence level, experiments are run multiple times to consider the variance because of
# randomness. This method aggregates the results for same method and confidence level and takes an average for the
# other parameters(budget & computed return).
read.aggregate.data <- function(file.results){
  df <- read.csv(file=file.results, header=TRUE, sep=",")
  #sub.df <- df %>% group_by(confidence)#, method)# %>% mutate(budget = mean(budget), return = mean(return))
  sub.df <- df %>% select(confidence, return, method) %>% group_by(confidence, method) %>% summarise(return = mean(return))
  return(sub.df)
}

# from http://www.sumsar.net/blog/2014/03/a-hack-to-create-matrices-in-R-matlab-style/
# does string parsing: make sure to not use | for anything else but a row separator
qm<-function(...)
{
    # turn ... into string
    args<-deparse(substitute(rbind(cbind(...))))

    # create "rbind(cbind(.),cbind(.),.)" construct
    args<-gsub("\\|","),cbind(",args)

    # eval
    # marek: added envir=parent.frame(1) to access local function args
    eval(parse(text=args),envir=parent.frame())
}

#' Generates samples from the probability distribution
#' @param p Probabilities
#' @param  nsamples Number of samples
#'
#' @return Vector with the number of samples for every state
generate.samples <- function(p, nsamples){
    c(rmultinom(1,nsamples,p))
}

#' Samples from the dirichlet posterior assuming a uniform prior
#'
#' @param tran.samples Samples as generated by generate.sample
#' @param prior Prior distribution. Vector of the same dimension as tran.samples
#' @param nposterior Number of samples from the posterior
sample.posterior.dirichlet <- function(tran.samples, prior, nposterior){
    require(gtools)
    alpha <- tran.samples + prior
    list(samples = rdirichlet(nposterior, alpha),
         mean = alpha / sum(alpha))
}




#' Computes weights from valuefunctions, for weighted Linf and L1
#' ambiguity set.
#'
#' @param z value function
#' @param norm ambiguity set's norm: "L1" or"Linf"
#' @param solution solving approach: "analytical" or "socp.marek" or "socp.clint"
compute.weights <-
    function(z,
             norm = "L1",
             solution = "analytical",
             p_bar = NULL,
             psi = NULL) {
        if (solution == "analytical") {
            w <- compute.weights.analytical(z, norm)
        } else if (solution == "socp" & norm == "L1") {
            w <- compute.weights.socp.marek(z, p_bar, psi)
        } else if (solution == "socp.clint" & norm == "L1") {
          w <- compute.weights.socp.clint(z, p_bar, psi)
        }
        return(w)
    }


#' Computes weights from valuefunctions, following the analytical solution for
#' weighted Linf and L1 ambiguity set.
#'
#' @param z value function
#' @param norm ambiguity set's norm: "L1" or"Linf"
compute.weights.analytical <- function(z, norm){
  if (norm == "L1"){
    lambda.bar <- (max(z) + min(z))/2
    b <- abs(z-lambda.bar)
    w <-   b / sqrt(sum(b^2))
  } else if (norm == "Linf") {
    lambda.bar = median(z)
    b <- abs(z-lambda.bar)
    w <- b^(1/3) / sqrt(sum(b^(2/3)))
  }
  eps <- 10^-9
  # add epsilon to avoid division by zero
  return (w + eps)
}




#' Computes weights from valuefunctions, and nominal transition probabilities
#'  following the SOCP formulation by Marek Petrik
#'  weighted L1 ambiguity set.
#'  maximize_{w,alpha,beta,gamma,nu}
#  -alpha\tr * p_bar + beta\tr p_bar - gamma * psi -nu
# s.t.
#        z + alpha - beta + nu * ones   >= 0
#        -alpha - beta + gamma * w  = 0
#        w\tr * w  <= 1
#        alpha in R_+^S,  beta in R_+^S,  gamma in R_+,   nu in R, w in R_++^S
#
# Marek's method
#
# maximize_{v,alpha,beta,gamma,nu}
#        -alpha\tr * p_bar + beta\tr p_bar - gamma * psi -nu     <- (linear)
# s.t.
#        z + alpha - beta + nu * ones   >= 0                      <- (linear)
#        - alpha - beta + v  = 0                                  <- (linear)  v = gamma * w
#        v\tr * v  <= S* gamma^2                                    <- (convex quadratic)
#        alpha in R_+^S,  beta in R_+^S,  gamma in R_+,   nu in R, w in R_++^S
#'
#' @param z value function
#' @param p_bar ambiguity set's norm: "L1" or"Linf"
#' @param psi budget of the amb set
compute.weights.socp.marek <- function(z, p_bar, psi) {
    epsilon <- 1e-6
    nStates <- length(p_bar)

    stopifnot(abs(sum(p_bar) - 1) < epsilon)
    stopifnot(length(z) == nStates)

    model <- list()

    # The coefficient for the objective function in (2)
    # v = gamma * w
    # Variables: c(alpha, beta, gamma, nu, v)
    model$obj <- c(-p_bar, p_bar,-psi,-1, rep(0, nStates))
    nVars <- length(model$obj)

    # Sparse matrix of I: nStates by nStates
    eye = spMatrix(
        nStates,
        nStates,
        i = c(1:nStates),
        j = (1:nStates),
        x = rep(1, nStates)
    )
    # Sparse matrix  of -1*I: nStates by nStates
    neg_eye = -eye
    # Sparse matrix zeros: nStates by nStates
    zeros = 0*eye

    # First linear constraint: alpha - beta + nu*1 >= -z
    A1 <-
        cbind(eye, neg_eye, rep(0, nStates), rep(1, nStates), zeros)
    rhs1 <- -z
    sense1 <- rep('>', nStates)
    # Second linear constraint: -alpha - beta + v = 0
    A2 <-
        cbind(neg_eye, neg_eye, rep(0, nStates), rep(0, nStates), eye)
    rhs2 <- rep(0, nStates)
    sense2 <- rep('=', nStates)


    # quadratic constraint: v\tr v - nStates* gamma^2 <= 0
    qc1 <- list()
    coef_quadratic <-
        c(rep(0, nStates), rep(0, nStates),-nStates, 0, rep(1, nStates))
    qc1$Qc <-
        spMatrix(nVars,
                 nVars,
                 i = c(1:nVars),
                 j = (1:nVars),
                 coef_quadratic)
    qc1$rhs <- 0.0

    model$A          <- rbind(A1, A2)
    model$rhs        <- c(rhs1, rhs2)
    model$sense      <- c(sense1, sense2)
    model$modelsense <- 'max'
    model$lb          <-
        c(rep(0, nStates), rep(0, nStates), 0 ,-Inf, rep(0, nStates))
    model$quadcon <- list(qc1)


    params <- list(OutputFlag = 0)

    result <- gurobi(model, params)

    gamma_index <- 2 * nStates + 1
    gamma <- result$x[gamma_index]

    v_index_start <- 2 * nStates + 3
    v_index_end <- nVars
    v <- result$x[c(v_index_start:v_index_end)]
    w <- v / gamma

    # Clear space
    rm(model, result, params)

    return(w)
}





#' Computes weights from valuefunctions, and nominal transition probabilities
#'  following the SOCP formulation by Chin Pang HO (Clint)
#'
# Clint's method
#
# maximize_{x,gamma,nu}
#         p_bar \tr x - psi * gamma - nu
#
# s.t.
#         z - x  + nu * ones >= 0
#         x\tr x <= gamma ^2
#         x in R^S
#         gamma in R+
#         nu  in R
#'
#' @param z value function
#' @param p_bar ambiguity set's norm: "L1" or"Linf"
#' @param psi budget of the amb set
compute.weights.socp.clint <- function(z, p_bar, psi) {
  epsilon <- 1e-6
  nStates <- length(p_bar)
  stopifnot(abs(sum(p_bar) - 1) < epsilon)
  stopifnot(length(z) == nStates)

  model <- list()

  # The coefficient for the objective function
  # Variables: c(x, gamma, nu)
  model$obj <- c(p_bar,-psi, -1)
  nVars <- length(model$obj)

  # Sparse matrix  of -1*I: nStates by nStates
  neg_eye = spMatrix(nStates, nStates, i = c(1:nStates), j = (1:nStates), x = rep(-1,nStates))

  # first linear constraint:  - x + nu * ones  >= -z
  A <- cbind(neg_eye, rep(0,nStates), rep(1, nStates))
  rhs <- -z
  sense <- rep('>', nStates)


  # quadratic constraint: x\tr x - gamma^2 <= 0
  qc1 <- list()
  coef_quadratic <- c(rep(1, nStates),-1, 0)

  qc1$Qc <- spMatrix(nVars, nVars, i = c(1:nVars), j = (1:nVars), coef_quadratic)
  qc1$rhs <- 0.0


  model$A          <- A
  model$rhs        <- rhs
  model$sense      <- sense
  model$modelsense <- 'max'
  model$lb          <- c(rep(- Inf, nStates), 0 , - Inf)

  model$quadcon <- list(qc1)

  params <- list(OutputFlag=0)

  result <- gurobi(model, params)


  gamma <- result$x[nStates+1]
  x <- result$x[1:nStates]
  # w_i = abs(x_i) / gamma
  w <- abs(x)/ gamma
   # Clear space
  rm(model, result, params)
  return(w)
}






#' Computes the size of the L1 Hoeffding set for a state in an MDP
#' with a given number of states.
#'
#' @param nsamples Number of samples available for the state.
#' @param nstates Number of states in the MDP
#' @param nactions Number of actions in the MDP for each state (average if varies from state to state)
#' @param confidence Desired confidence level: 1-delta
l1.size.hoeffding <- function(nsamples, nstates, nactions, confidence){
    sqrt(2/nsamples*log(nstates * nactions * (2^nstates - 2) / (1-confidence)))
}

#' Computes the size of the L1 Hoeffding set for a state in an MDP
#' with a given number of states.
#'
#' @param nsamples Number of samples available for the state.
#' @param nstates Number of states in the MDP
#' @param nactions Number of actions in the MDP for each state (average if varies from state to state)
#' @param confidence Desired confidence level: 1-delta
l1.size.bernstein <- function(nsamples, nstates, nactions, confidence, weights){
  #x = log( (2^nstates - 2)/(1-confidence) )

  #m = max(weights)
  #( 4*m*x + sqrt(16*m*m*x*x + 4*3*nsamples*6*m*m*x) ) / (2*3*nsamples)
  #confidence = 1 - confidence
  ( 4*confidence+sqrt(16*confidence*confidence+72*nsamples*confidence) ) / 6*nsamples
}



#' Computes the Hoeffding confidence if the ambiguity set is
#' bounded by the weighted L1 norm, with `nsamples`` samples
#' with weights `weights` of size `psi`
compute.delta.l1 <- function(psi, nsamples, weights.sorted){

    S <- length(weights.sorted)
    prob <- 0
    S_ <- S - 1
    for(i in 1:S_){
        prob <- prob + 2^(S-i) * exp(- psi^2 * nsamples / (2 * weights.sorted[i]^2));
    }
    return (prob)
}

#' Computes the size of the weighted L1 Hoeffding set
#'
#' Assumes that weights are at most 2
#' @param nsamples Number of samples
#' @param nstates Number of states
#' @param nactions Number of actions (in the MDP)
#' @param confidence 1-delta
#' @param weights Vector of state weights (0 <= weights)

l1.weighted.size.hoeffding <- function(nsamples, nstates, nactions, confidence, weights){
    stopifnot(min(weights) > 0)
    stopifnot(length(weights) == nstates)
    #stopifnot(nactions == 1)


    # the maximum size of the ambiguity set with weights <= 2
    # see the assumption above
    upper.bound <- 4.0
    weights.sorted <- sort(weights, decreasing = TRUE)

    # update confidence for each state and action
    delta <- (1-confidence)
    delta <- delta /(nstates * nactions)

    #cat("l1.weighted.size.hoeffding, delta: ", delta,"\n")

    fun.delta <- function(psi) {
        compute.delta.l1(psi, nsamples, weights.sorted) - delta
    }
    # check that the upper bound on the size is sufficient
    # to achieve the desired confidence
    if(fun.delta(upper.bound) >= delta) {

        return (upper.bound)
    }

    psi.sol <- uniroot(fun.delta, c(0, upper.bound))
    return (psi.sol$root)
}


#' Computes the Bernstein confidence if the ambiguity set is
#' bounded by the weighted L1 norm, with `nsamples`` samples
#' with weights `weights` of size `psi`
compute.delta.l1.bernstein <- function(psi, nsamples, weights.sorted){

  S <- length(weights.sorted)
  prob <- 0
  S_ <- S - 1
  for(i in 1:S_){
      prob <- prob +  2^(S-i) * exp(- 3*psi^2 * nsamples / ( 6*weights.sorted[i]^2 + 4*weights.sorted[i] * psi));
  }
  return (prob)
}

#' Computes the size of the weighted L1 Bernstein set
#'
#' Assumes that weights are at most 2
#' @param nsamples Number of samples
#' @param nstates Number of states
#' @param nactions Number of actions (in the MDP)
#' @param confidence 1-delta
#' @param weights Vector of state weights (0 <= weights)

l1.weighted.size.bernstein <- function(nsamples, nstates, nactions, confidence, weights){
  stopifnot(min(weights) > 0.0)
  stopifnot(length(weights) == nstates)
  #stopifnot(nactions == 1)

  # the maximum size of the ambiguity set with weights <= 2
  # see the assumption above
  upper.bound <- 4.0
  weights.sorted <- sort(weights, decreasing = TRUE)


  # update confidence for each state and action
  delta <- (1-confidence)
  delta <- delta /(nstates * nactions)

  #cat("l1.weighted.size.bernstein, delta: ", delta,"\n")

  fun.delta <- function(psi) {
    compute.delta.l1.bernstein(psi, nsamples, weights.sorted) - delta
  }
  # check that the upper bound on the size is sufficient
  # to achieve the desired confidence
  if(fun.delta(upper.bound) >= (1-confidence)) {
    return (upper.bound)
  }

  psi.sol <- uniroot(fun.delta, c(0, upper.bound))
  return (psi.sol$root)
}

#' Computes the size of the Linf Hoeffding set
#'
#' @param nsamples Number of samples
#' @param nstates Number of states
#' @param nactions Number of actions (in the MDP)
#' @param confidence 1-delta
linf.size.hoeffding <- function(nsamples, nstates, nactions, confidence){
  #stopifnot(nactions == 1)
  delta <- (1-confidence)
  delta <- delta /(nstates * nactions)
  sqrt((1/(2*nsamples))*log(nstates * 2 / delta))
}


#' Computes the size of the Linf Hoeffding set
#'
#' @param nsamples Number of samples
#' @param nstates Number of states
#' @param nactions Number of actions (in the MDP)
#' @param confidence 1-delta
linf.wmax.size.hoeffding <- function(nsamples, nstates, nactions, confidence, weights){
  stopifnot(nactions == 1)
  delta <- (1-confidence)
  delta <- delta /(nstates * nactions)
  w_max = max(weights)

  sqrt((w_max^2/(2*nsamples))*log(nstates * 2 / delta))
}




#' Computes the Hoeffding confidence if the ambiguity set is
#' bounded by the weighted Linf norm, with `nsamples`` samples
#' with weights `weights` of size `psi`
compute.delta.linf <- function(psi, nsamples, weights){

  S <- length(weights)
  prob <- 0
  for(i in 1:S){
    prob <- prob + 2 * exp(- 2 * psi^2 * nsamples / (weights[i]^2));
  }
  return (prob)
}


#' Computes the size of the weighted Linf Hoeffding set
#'
#' Assumes that weights are at most 2
#' @param nsamples Number of samples
#' @param nstates Number of states
#' @param nactions Number of actions (in the MDP)
#' @param confidence 1-delta
#' @param weights Vector of state weights (0 <= weights)

linf.weighted.size.hoeffding <- function(nsamples, nstates, nactions, confidence, weights){
  stopifnot(min(weights) > 0)
  stopifnot(length(weights) == nstates)
  #stopifnot(nactions == 1)

  # the maximum size of the ambiguity set with weights <= 2
  # see the assumption above
  upper.bound <- 4.0

  # update confidence for each state and action
  delta <- (1-confidence)
  delta <- delta /(nstates * nactions)

  #cat("linf.weighted.size.hoeffding, delta: ", delta,"\n")

  fun.delta <- function(psi) {
    compute.delta.linf(psi, nsamples, weights) - delta
  }
  # check that the upper bound on the size is sufficient
  # to achieve the desired confidence
  if(fun.delta(upper.bound) >= delta) {
    return (upper.bound)
  }

  psi.sol <- uniroot(fun.delta, c(0, upper.bound))

  # test bisection
  delta_check <- 0
  S <- length(weights)
  for(i in 1:S){
    delta_check <- delta_check + 2 * exp(- psi.sol$root^2 * nsamples / (weights[i]^2));
  }
  #cat("delta:" , delta_check, delta,"\n")
  #stopifnot(abs(delta_check - delta) < eps )
  return (psi.sol$root)
}



#' Computes the size psi of an l1 set for the given nominal point,
#' posterior samples, and the confidence level. More formally
#' $P_p( || p - nominal || >= psi) <= 1 - confidence$
#' where p are the elements of post.sample
#'
#' @param nominal 3D vector of the nominal point
#' @param post.samples Matrix, 3 columns, each row is a sample from
#'                from the posterior
#' @param nactions Theoretical number of actions, used to adjust confidence
#' @param confidence A confidence level between 0.0 and 1.0
#' @param weights Vector of weights for the features. A scalar value is applied uniformly
l1.size.bayes <- function(nominal, post.samples, nactions, confidence, weights = 1){

    norm.fun <-
        if(is.scalar(weights)){
            function(x){ weights * sum(abs(x - nominal)) }
        }else{
            function(x){ weights %*% abs(x - nominal) }
        }

    distances <- apply(post.samples, 1, norm.fun)
    nstates <- length(nominal)
    as.numeric(quantile(distances, 1-(1-confidence)/nactions/nstates))
}

#' Computes the size psi of an linfty set for the given nominal point,
#' posterior samples, and the confidence level. More formally
#' $P_p( || p - nominal || >= psi) <= 1 - confidence$
#' where p are the elements of post.sample
#'
#' @param nominal 3D vector of the nominal point
#' @param post.samples Matrix, 3 columns, each row is a sample from
#'                from the posterior
#' @param nactions Theoretical number of actions, used to adjust confidence
#' @param confidence A confidence level between 0.0 and 1.0
#' @param weights Vector of weights for the features. A scalar value is applied uniformly
linf.size.bayes <- function(nominal, post.samples, nactions, confidence, weights = 1){

    norm.fun <-
        if(is.scalar(weights)){
            function(x){ weights * max(abs(x - nominal)) }
        }else{
            function(x){ max(weights * abs(x - nominal)) }
        }

    distances <- apply(post.samples, 1, norm.fun)
    nstates <- length(nominal)
    as.numeric(quantile(distances, 1-(1-confidence)/nactions/nstates))
}

#' Computes the optimal half-space ambiguity set for a given objective function
#' psi such that:
#' $P_p(obj^T p <= psi) <= 1 - confide  nce$
#' where p are the elements of post.sample
#'
#' The half-space is defined by:
#' obj^T p <= psi
#'
#' @param post.samples Matrix, 3 columns, each row is a sample from
#'                from the posterior
#'
#' @param obj           A 3D objective function for the optimal hyperplane
#' @param objs          A matrix of objectives for the RSVF algorithm
#' @param nactions      Theoretical number of actions, used to adjust confidence
#' @param confidence    A confidence level between 0.0 and 1.0
#'
#' @return Value psi
halfspace.bayes <- function(post.samples, objective, nstates, nactions, confidence){
    as.numeric(quantile(post.samples %*% objective, (1-confidence)/nstates/nactions))
}


#' Computes the value at risk at level alpha for a given random sample
#'
#' @param vals Random sample of the values
#' @param alpha Risk level
risk.var <- function(vals, alpha){
    return(as.numeric(quantile(vals, (1-alpha))))
}

#' Computes a convex lower bound approximation on the value at risk
#' Based on the derivation in Gupta 2015: Near-Optimal Ambiguity Sets
#' Page 13, Eq (12)
#'
#' z^t u_N - sqrt(alpha / (1-alpha)) z^T Sigma z
#' where u_N is the mean and Sigma is the covariance matrix
#'
risk.var.conv <- function(vars, alpha){
    mean(vars) - sqrt(alpha/(1-alpha))  * sd(vars)
}


#' Computes the average value at risk (AVaR) at level alpha for a given random sample
#'
#' AVaR is also known as conditional value at risk (CVaR) or expected shortfall (ES)
#'
#' @param vals Random sample of the values
#' @param alpha Risk level
risk.avar <- function(vals, alpha){
    # compute the quantile
    mean(vals[vals <= risk.var(vals, alpha)])
}

#' Computes the optimal half-space ambiguity set for a given objective function
#' psi and a risk measure rho_confidence such that the following value is
#' the maximal possible:
#' $ alpha * rho_confidence[ obj^T p ] + (1-alpha) E[ obj^T p]$
#' where p are the elements of post.sample
#'
#' The half-space is defined by:
#' obj^T p <= psi
#'
#' @param post.samples Matrix, 3 columns, each row is a sample from
#'                from the posterior
#'
#' @param obj           A 3D objective function for the optimal hyperplane
#' @param objs          A matrix of objectives for the RSVF algorithm
#' @param nactions      Theoretical number of actions, used to adjust confidence
#' @param confidence    A confidence level between 0.0 and 1.0
#' @param risk.function The function that measures the risk, or robustness, VaR or AVaR
#' @param beta          The risk is a convex combination of the actual risk
#'                      and the expected mean value
#'
#' @return Value psi
optset.bayes <- function(post.samples, objective, confidence,
                            risk.function = "VaR", beta = 1){
    vals <- post.samples %*% objective
    if(risk.function == "VaR"){
        return(beta * risk.var(vals, confidence) + (1-beta) * mean(vals))
    }else if(risk.function == "VaR.Conv"){
            return(beta * risk.var.conv(vals, confidence) + (1-beta) * mean(vals))
    }else if(risk.function == "AVaR"){
        return(beta * risk.avar(vals, confidence) + (1-beta) * mean(vals))
    }else{
        stop("Unknown risk function")
    }

}







#' Plots Single Bellman Update graphs
#'
#' @param df.data data frame of c(return, budget, method, confidence)
#' @param file.name
plot.res.color <- function(df.data, file.name) {
  # Draw a plot with the colour legend
  (
    p1 <- ggplot(df.data, aes(
      x = confidence, y = return, group = method
    )) +
      geom_point(aes(color = budget)) +
      theme_light() +
      scale_color_gradient(
        low = "blue",
        high = "green",
        name = TeX("Budget: $\\psi$")
      ) +
      theme(
        legend.position = "right",
        legend.direction = "vertical",
        legend.box = "vertical",
        legend.background
        = element_rect(
          fill = "gray95",
          size = 0.2,
          linetype = "solid",
          color = "black"
        )
      ) +
      scale_shape_manual(values = seq(0, length(method_names)))
  )

  # Extract the colour legend - leg1
  leg1 <-
    gtable_filter(ggplot_gtable(ggplot_build(p1)), "guide-box")

  # Draw a plot with the size legend
  (
    p2 <- ggplot(df.data, aes(
      x = confidence, y = return, group = method
    )) +
      geom_line(aes(linetype = method)) + geom_point(aes(shape = method)) +
      theme_light() +
      theme(
        legend.position = "top",
        legend.direction = "horizontal",
        legend.box = "horizontal",
        legend.background
        = element_rect(
          fill = "gray95",
          size = 0.2,
          linetype = "solid",
          color = "black"
        )
      ) +
      scale_shape_manual(values = seq(0, length(method_names))) +
      labs(shape = "Methods", linetype = "Methods")
  )

  # Extract the size legend - leg2
  leg2 <-
    gtable_filter(ggplot_gtable(ggplot_build(p2)), "guide-box")


  # Draw a plot with no legends - plot
  (
    plot <-
      ggplot(df.data, aes(
        x = confidence, y = return, group = method
      )) +
      geom_line(aes(linetype = method)) + geom_point(aes(color = budget, shape =
                                                           method), size = 2) +
      theme_light() +
      scale_color_gradient(
        low = "blue",
        high = "green",
        name = TeX("Budget: $\\psi$")
      ) +
      theme(legend.position = "none") +
      scale_shape_manual(values = seq(0, length(method_names))) +
      #geom_ribbon(aes( ymin=(return-return.sd),ymax=(return+return.sd) ),alpha=0.1) +
      labs(
        title = "",
        subtitle = "",
        y = TeX("Guaranteed Return"),
        x = TeX("Confidence Level: $1-\\delta$")
      )
  )

  lay <- rbind(c(1, 1, 1, 1, 1, 2),
               c(1, 1, 1, 1, 1, 2),
               c(1, 1, 1, 1, 1, 2),
               c(1, 1, 1, 1, 1, 2),
               c(3, 3, 3, 3, 3, 3))
  gg <- grid.arrange(
    plot,
    leg1,
    leg2,
    layout_matrix = lay,
    left = TeX("Guaranteed Return"),
    bottom = TeX("Confidence Level: $1-\\delta$")
  )

  ggsave(
    paste("figs/", file.name, ".pdf", sep = ""),
    plot = gg,
    width = 7,
    height = 5,
    device = 'pdf'
  )
  plot(gg)
}




