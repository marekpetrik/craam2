% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{rsolve_mdpo_sa}
\alias{rsolve_mdpo_sa}
\title{Solves a robust Markov decision process with state-action rectangular
ambiguity sets.}
\usage{
rsolve_mdpo_sa(
  mdpo,
  discount,
  nature,
  nature_par,
  algorithm = "mppi",
  policy_fixed = NULL,
  maxresidual = 0.001,
  iterations = 10000L,
  timeout = 300,
  value_init = NULL,
  pack_actions = FALSE,
  output_tran = FALSE,
  show_progress = 1L
)
}
\arguments{
\item{algorithm}{One of "ppi", "mppi", "mpi", "vi", "vi_j", "vi_g", "pi". MPI may
may not converge}

\item{policy_fixed}{States for which the  policy should be fixed. This
should be a dataframe with columns idstate and idaction. The policy
is optimized only for states that are missing, and the fixed policy
is used otherwise}

\item{maxresidual}{Residual at which to terminate}

\item{iterations}{Maximum number of iterations}

\item{timeout}{Maximum number of secods for which to run the computation}

\item{value_init}{A  dataframe that contains the initial value function used
to initialize the method. The columns should be idstate and value.
Any states that are not provided are initialized to 0.}

\item{pack_actions}{Whether to remove actions with no transition probabilities,
and rename others for the same state to prevent gaps. The policy
for the original actions can be recovered using ``action_map'' frame
in the result}

\item{output_tran}{Whether to construct and return a matrix of transition
probabilites and a vector of rewards}

\item{show_progress}{Whether to show a progress bar during the computation.
0 means no progress, 1 is progress bar, and 2 is a detailed report}
}
\value{
A list with value function policy and other values
}
\description{
The worst-case is computed across the outcomes and not
the actual transition probabilities.
}
\details{
NOTE: The algorithms  mpi and pi may cycle infinitely without converging to a solution,
when solving a robust MDP.
The algorithms ppi and mppi are guaranteed to converge to an optimal solution.



The options for nature and the corresponding nature_par are:
   \itemize{
        \item "exp" plain expectation over the outcomes
        \item "evaru" a convex combination of expectation and V@R over
                transition probabilites. Uniform over all states and actions
                nature_par is a list with parameters (alpha, beta). The worst-case
                response is computed as:
                beta * var [z] + (1-beta) * E[z], where
                var is \eqn{VaR(z,\alpha) = \inf{x \in R : P[X <= x] >= \alpha}}, with \eqn{\alpha = 0} being the
                worst-case.
        \item "eavaru" a convex combination of expectation and AV@R over
                transition probabilites. Uniform over states
                nature_par is a list with parameters (alpha, beta). The worst-case
                response is computed as:
                beta * avar [z] + (1-beta) * E[z], where
                avar is \eqn{AVaR(z,alpha) =  1/alpha * ( E[X I{X <= x_a} ] + x_a (alpha - P[X <= x_a] )}
                where I is the indicator function and
                \eqn{x_a = \inf{x \in R : P[X <= x] >= \alpha}} being the
                worst-case.
   }
}
