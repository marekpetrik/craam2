% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{rsolve_mdp_sa}
\alias{rsolve_mdp_sa}
\title{Solves a robust Markov decision process with state-action rectangular
ambiguity sets. The worst-case is computed with the MDP transition
probabilities treated as nominal values.}
\usage{
rsolve_mdp_sa(mdp, discount, nature, nature_par, algorithm = "mppi",
  policy_fixed = NULL, maxresidual = 0.001, iterations = 10000L,
  timeout = 300, pack_actions = FALSE, output_tran = FALSE,
  show_progress = TRUE)
}
\arguments{
\item{mdp}{A dataframe representation of the MDP. Each row
represents a single transition from one state to another
after taking an action a. The columns are:
idstatefrom, idaction, idstateto, probability, reward}

\item{discount}{Discount factor in [0,1]}

\item{nature}{Algorithm used to select the robust outcome. See details for options.}

\item{nature_par}{Parameters for the nature. Varies depending on the nature.
See details for options.}

\item{algorithm}{One of "ppi", "mppi", "mpi", "vi", "vi_j", "pi". MPI and PI may
may not converge}

\item{policy_fixed}{States for which the  policy should be fixed. This
should be a dataframe with columns idstate and idaction. The policy
is optimized only for states that are missing, and the fixed policy
is used otherwise}

\item{maxresidual}{Residual at which to terminate}

\item{iterations}{Maximum number of iterations}

\item{timeout}{Maximum number of secods for which to run the computation}

\item{pack_actions}{Whether to remove actions with no transition probabilities,
and rename others for the same state to prevent gaps. The policy
for the original actions can be recovered using ``action_map'' frame
in the result}

\item{output_tran}{Whether to construct and return a matrix of transition
probabilites and a vector of rewards}

\item{show_progress}{Whether to show a progress bar during the computation}
}
\value{
A list with value function policy and other values
}
\description{
NOTE: The algorithms: pi, mpi may cycle infinitely without converging to a solution,
when solving a robust MDP.
The algorithms ppi and mppi are guaranteed to converge to an optimal solution.
}
\details{
Important: Worst-case transitions are allowed only to idstateto states that
are provided in the mdp dataframe, even when the transition
probability to those states is 0.



The options for nature and the corresponding nature_par are:
   \itemize{
        \item "l1u" an l1 ambiguity set with the same budget for all s,a.
               nature_par is a float number representing the budget
        \item "l1" an ambiguity set with different budgets for each s,a.
               nature_par is dataframe with idstate, idaction, budget
        \item "l1w" an l1-weighted ambiguity set with different weights
                     and budgets for each state and action
                nature_par is a list with two elements: budgets, weights.
                budgets must be a dataframe with columns idstate, idaction, budget
                and weights must be a dataframe with columns:
                idstatefrom, idaction, idstateto, weight (for the l1 weighted norms)
        \item "evaru" a convex combination of expectation and V@R over
                transition probabilites. Uniform over all states and actions
                nature_par is a list with parameters (alpha, beta). The worst-case
                response is computed as:
                beta * var [z] + (1-beta) * E[z], where
                var is inf{x \in R : P[X <= x] >= alpha}, with alpha = 0 being the
                worst-case.
        \item "evaru" a convex combination of expectation and AV@R over
                transition probabilites. Uniform over states
                nature_par is a list with parameters (alpha, beta). The worst-case
                response is computed as:
                beta * var [z] + (1-beta) * E[z], where
                var is AVaR(z,alpha) =  1/alpha * ( E[X I{X <= x_a} ] + x_a (alpha - P[X <= x_a] )
                where I is the indicator function and
                x_a = inf{x \in R : P[X <= x] >= alpha} being the
                worst-case.
   }
}
